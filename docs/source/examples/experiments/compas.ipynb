{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# COMPAS - Bias\n",
    "This notebook computes the racial bias of COMPAS decile score using different metrics."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fairscoring.metrics import bias_metric_pe, bias_metric_eo, bias_metric_cal, \\\n",
    "    WassersteinMetric, CalibrationMetric\n",
    "from fairscoring.metrics.roc import bias_metric_roc, bias_metric_xroc\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:18.165052100Z",
     "start_time": "2024-08-07T12:09:16.827281800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting\n",
    "### Load COMPAS data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataURL = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "df = pd.read_csv(dataURL)\n",
    "\n",
    "df.rename(columns=dict((column_name, column_name.lower()) for column_name in df.columns),\n",
    "          inplace=True)\n",
    "\n",
    "score_column = 'decile_score'\n",
    "target_column = 'two_year_recid'\n",
    "protected_attribute_column = 'race'\n",
    "\n",
    "# Get Columns\n",
    "scores = df[score_column]\n",
    "target = df[target_column]\n",
    "attribute = df[protected_attribute_column]\n",
    "\n",
    "# Groups to compare\n",
    "groups = ['African-American', 'Caucasian']\n",
    "# groups = ['African-American', None]    # None = all others\n",
    "\n",
    "favorable_target = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:18.564438500Z",
     "start_time": "2024-08-07T12:09:18.169072700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### List of bias metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    bias_metric_eo,     # Standardized Equal Opportunity\n",
    "    bias_metric_pe,     # Standardized Predictive Equality\n",
    "    bias_metric_cal,    # Standardized Calibration Equality\n",
    "    bias_metric_roc,    # ROC-Bias\n",
    "    bias_metric_xroc,   # xROC-Bias\n",
    "    WassersteinMetric(fairness_type=\"EO\",name=\"Equal Opportunity (U)\", score_transform=\"rescale\"),\n",
    "    WassersteinMetric(fairness_type=\"PE\",name=\"Predictive Equality (U)\", score_transform=\"rescale\"),\n",
    "    CalibrationMetric(weighting=\"scores\",name=\"Calibration (U)\", score_transform=\"rescale\"),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:18.580397300Z",
     "start_time": "2024-08-07T12:09:18.567431500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bias Measures\n",
    "### Compute Bias Table\n",
    "Compute all bias metrics for the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e1b3d0d1fc845808784526319ec4fe1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\dev\\fair-scoring-public\\src\\fairscoring\\metrics\\calibration.py:81: RuntimeWarning: invalid value encountered in divide\n",
      "  fraction_of_positives = np.where(nonzero, bin_true / bin_total, np.nan)\n",
      "C:\\dev\\fair-scoring-public\\src\\fairscoring\\metrics\\calibration.py:82: RuntimeWarning: invalid value encountered in divide\n",
      "  mean_predicted_value = np.where(nonzero, bin_sums / bin_total, np.nan)\n",
      "C:\\dev\\fair-scoring-public\\src\\fairscoring\\metrics\\calibration.py:81: RuntimeWarning: invalid value encountered in divide\n",
      "  fraction_of_positives = np.where(nonzero, bin_true / bin_total, np.nan)\n",
      "C:\\dev\\fair-scoring-public\\src\\fairscoring\\metrics\\calibration.py:82: RuntimeWarning: invalid value encountered in divide\n",
      "  mean_predicted_value = np.where(nonzero, bin_sums / bin_total, np.nan)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for metric in tqdm(metrics):\n",
    "    # Compute bias\n",
    "    bias = metric.bias(\n",
    "        scores, target, attribute,\n",
    "        groups=groups,\n",
    "        favorable_target=favorable_target,\n",
    "        min_score=1, max_score=10,\n",
    "        n_permute=1000, seed=2579,\n",
    "        prefer_high_scores=False\n",
    "    )\n",
    "\n",
    "    # Store result\n",
    "    results.append((metric, bias))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:29.611284200Z",
     "start_time": "2024-08-07T12:09:18.584385900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Result Table\n",
    "This corresponds to Tab. 1 and Tab. C1 in the publication."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                         total  pos   neg p-value\nmetric                                           \nEqual Opportunity        0.161   0%  100%    0.00\nPredictive Equality      0.154   0%  100%    0.00\nCalibration              0.034  79%   21%    0.30\nROC bias                 0.016  46%   54%    0.31\nxROC bias                0.273   0%  100%    0.00\nEqual Opportunity (U)    0.152   0%  100%    0.00\nPredictive Equality (U)  0.163   0%  100%    0.00\nCalibration (U)          0.037  78%   22%    0.23",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>total</th>\n      <th>pos</th>\n      <th>neg</th>\n      <th>p-value</th>\n    </tr>\n    <tr>\n      <th>metric</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Equal Opportunity</th>\n      <td>0.161</td>\n      <td>0%</td>\n      <td>100%</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>Predictive Equality</th>\n      <td>0.154</td>\n      <td>0%</td>\n      <td>100%</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>Calibration</th>\n      <td>0.034</td>\n      <td>79%</td>\n      <td>21%</td>\n      <td>0.30</td>\n    </tr>\n    <tr>\n      <th>ROC bias</th>\n      <td>0.016</td>\n      <td>46%</td>\n      <td>54%</td>\n      <td>0.31</td>\n    </tr>\n    <tr>\n      <th>xROC bias</th>\n      <td>0.273</td>\n      <td>0%</td>\n      <td>100%</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>Equal Opportunity (U)</th>\n      <td>0.152</td>\n      <td>0%</td>\n      <td>100%</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>Predictive Equality (U)</th>\n      <td>0.163</td>\n      <td>0%</td>\n      <td>100%</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>Calibration (U)</th>\n      <td>0.037</td>\n      <td>78%</td>\n      <td>22%</td>\n      <td>0.23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [[\n",
    "    metric.name,\n",
    "    f\"{bias.bias:.3f}\",\n",
    "    f\"{bias.pos_component:.0%}\",\n",
    "    f\"{bias.neg_component:.0%}\",\n",
    "    f\"{bias.p_value:.2f}\" ] for metric, bias in results\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"metric\", \"total\", \"pos\", \"neg\", \"p-value\"])\n",
    "df.set_index(\"metric\", inplace=True)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:29.652248100Z",
     "start_time": "2024-08-07T12:09:29.618233500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-07T12:09:29.711254800Z",
     "start_time": "2024-08-07T12:09:29.645730400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
